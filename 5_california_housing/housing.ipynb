{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing();\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already cleaned and ready for analysis, so let's split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.9074970979364\n",
      "-4176.427296584963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "lin_pred = lin_reg.predict(X_test)\n",
    "\n",
    "lin_rmse = mean_squared_error(y_test, lin_pred, squared=False)\n",
    "print(lin_rmse)\n",
    "print(r2_score(y_test, lin_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the targets represent hundreds of thousands of dollars, so this model gives us errors close to $73,900, which is pretty bad. Let's see if we can do better with other regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 17, 'solver': 'auto', 'tol': 0.0001}\n",
      "73.88327977044385\n",
      "-4173.690103355649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(random_state=17)\n",
    "\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "print(ridge_reg.get_params())\n",
    "\n",
    "ridge_pred = ridge_reg.predict(X_test)\n",
    "\n",
    "ridge_rmse = mean_squared_error(y_test, ridge_pred, squared=False)\n",
    "print(ridge_rmse)\n",
    "print(r2_score(y_test, ridge_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0, 'solver': 'svd'}\n",
      "73.88327977044385\n",
      "-4176.4272965849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"alpha\": [0, 100],\n",
    "    \"solver\": [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]\n",
    "}\n",
    "\n",
    "ridge_grid_search = GridSearchCV(ridge_reg, params, cv=10)\n",
    "ridge_grid_search.fit(X_train, y_train)\n",
    "\n",
    "ridge_grid_search_pred = ridge_grid_search.best_estimator_.predict(X_test)\n",
    "print(ridge_grid_search.best_params_)\n",
    "\n",
    "ridge_opt_rmse = mean_squared_error(y_test, ridge_pred, squared=False)\n",
    "print(ridge_opt_rmse)\n",
    "print(r2_score(y_test, ridge_grid_search_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid search doesn't seem to lead to any improvement in RMSE and R2 score of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using Random Forest Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 17, 'verbose': 0, 'warm_start': False}\n",
      "1.729568972940536\n",
      "-1.2877453527086837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=17)\n",
    "\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "model_params = rf_reg.get_params()\n",
    "print(model_params)\n",
    "\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "\n",
    "rf_rmse = mean_squared_error(y_test, rf_pred, squared=False)\n",
    "print(rf_rmse)\n",
    "print(r2_score(y_test, rf_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regression seems to be doing a lot better, even without tuning the hyperparameters. Using grid search to tune the params is going to be pretty slow so we'll try randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 261, 'criterion': 'poisson'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": np.arange(100, 400),\n",
    "    \"criterion\": [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"]\n",
    "}\n",
    "\n",
    "rf_rndm_search = RandomizedSearchCV(\n",
    "    rf_reg, params, n_iter=100, cv=3, random_state=17)\n",
    "rf_rndm_search.fit(X_train[:1000], y_train[:1000])\n",
    "\n",
    "rf_rndm_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9362738737556968\n",
      "-1.8672494711209158\n"
     ]
    }
   ],
   "source": [
    "rf_grid_search_pred = rf_rndm_search.best_estimator_.predict(X_test)\n",
    "\n",
    "rf_grid_search_rmse = mean_squared_error(\n",
    "    y_test, rf_grid_search_pred, squared=False)\n",
    "print(rf_grid_search_rmse)\n",
    "print(r2_score(y_test, rf_grid_search_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, but with the best params from randomized search, the model seems to be doing worse. TODO investigate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'poisson', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 261, 'n_jobs': None, 'oob_score': False, 'random_state': 17, 'verbose': 0, 'warm_start': False}\n",
      "1.6193545149778867\n",
      "-1.005468215170659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    criterion=\"poisson\", n_estimators=261, random_state=17)\n",
    "\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "model_params = rf_reg.get_params()\n",
    "print(model_params)\n",
    "\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "\n",
    "rf_rmse = mean_squared_error(y_test, rf_pred, squared=False)\n",
    "print(rf_rmse)\n",
    "print(r2_score(y_test, rf_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model with the best params from randomized search seems to lead to better results, possibly because randomized search was fitted on only 1000 instances (it's too slow to fit on all). A RMSE of 1.619 means that the model gives us an error of about $1,600, which is pretty decent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1be05c88609dd92a84858e070b094602778c145575f773bbd9160755b9e158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
